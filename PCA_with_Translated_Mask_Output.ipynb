{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tddqaRci1REJ",
    "tags": []
   },
   "source": [
    "# Determining the longest diagonal of masks from MaskRCNN-Translated output on C3S-only dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmOSGkSRQOLi"
   },
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORCZLvMGRziu"
   },
   "source": [
    "Import MaskRCNN code from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KV5hPx3jTBtm",
    "outputId": "0941830b-acde-4ff9-b409-dce7d96a339d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Mask-RCNN-TF2.8'...\n",
      "remote: Enumerating objects: 836, done.\u001b[K\n",
      "remote: Counting objects: 100% (134/134), done.\u001b[K\n",
      "remote: Compressing objects: 100% (104/104), done.\u001b[K\n",
      "remote: Total 836 (delta 46), reused 63 (delta 30), pack-reused 702\u001b[K\n",
      "Receiving objects: 100% (836/836), 135.44 MiB | 1.72 MiB/s, done.\n",
      "Resolving deltas: 100% (470/470), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone 'https://github.com/Rene-Michel99/Mask-RCNN-TF2.8.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "wn_9q0GVAww2"
   },
   "outputs": [],
   "source": [
    "!mv ./Mask-RCNN-TF2.8/* ./\n",
    "!rm -rf ./Mask-RCNN-TF2.8\n",
    "!rm -rf assets\n",
    "!rm -rf tests\n",
    "\n",
    "# TODO: solve import error of load_image_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_inVTEWA-_C"
   },
   "source": [
    "Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Jmjo_57jAytK",
    "outputId": "9f9427ed-be16-4ed2-db2a-0d44c88f5c89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow==2.9.1 in /home/lazuli/.local/lib/python3.10/site-packages (from -r ./requirements.txt (line 1)) (2.9.1)\n",
      "Requirement already satisfied: scikit-image==0.20.0 in /home/lazuli/.local/lib/python3.10/site-packages (from -r ./requirements.txt (line 2)) (0.20.0)\n",
      "Requirement already satisfied: opencv-python in /home/lazuli/.local/lib/python3.10/site-packages (from -r ./requirements.txt (line 3)) (4.8.0.76)\n",
      "Requirement already satisfied: numpy in /home/lazuli/.local/lib/python3.10/site-packages (from -r ./requirements.txt (line 4)) (1.23.5)\n",
      "Requirement already satisfied: imgaug in /home/lazuli/.local/lib/python3.10/site-packages (from -r ./requirements.txt (line 5)) (0.4.0)\n",
      "Requirement already satisfied: pycocotools in /home/lazuli/.local/lib/python3.10/site-packages (from -r ./requirements.txt (line 6)) (2.0.6)\n",
      "Requirement already satisfied: Pillow==9.0.1 in /home/lazuli/.local/lib/python3.10/site-packages (from -r ./requirements.txt (line 7)) (9.0.1)\n",
      "Requirement already satisfied: imutils in /home/lazuli/.local/lib/python3.10/site-packages (from -r ./requirements.txt (line 8)) (0.5.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow==2.9.1->-r ./requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /home/lazuli/.local/lib/python3.10/site-packages (from tensorflow==2.9.1->-r ./requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/lazuli/.local/lib/python3.10/site-packages (from tensorflow==2.9.1->-r ./requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/lazuli/.local/lib/python3.10/site-packages (from tensorflow==2.9.1->-r ./requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/lazuli/.local/lib/python3.10/site-packages (from tensorflow==2.9.1->-r ./requirements.txt (line 1)) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /home/lazuli/.local/lib/python3.10/site-packages (from tensorflow==2.9.1->-r ./requirements.txt (line 1)) (2.9.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/lib/python3/dist-packages (from tensorflow==2.9.1->-r ./requirements.txt (line 1)) (1.30.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/lazuli/.local/lib/python3.10/site-packages (from tensorflow==2.9.1->-r ./requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/lazuli/.local/lib/python3.10/site-packages (from tensorflow==2.9.1->-r ./requirements.txt (line 1)) (16.0.6)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/lazuli/.local/lib/python3.10/site-packages (from tensorflow==2.9.1->-r ./requirements.txt (line 1)) (0.33.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/lazuli/.local/lib/python3.10/site-packages (from tensorflow==2.9.1->-r ./requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/lazuli/.local/lib/python3.10/site-packages (from tensorflow==2.9.1->-r ./requirements.txt (line 1)) (4.7.1)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /home/lazuli/.local/lib/python3.10/site-packages (from tensorflow==2.9.1->-r ./requirements.txt (line 1)) (2.9.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/lazuli/.local/lib/python3.10/site-packages (from tensorflow==2.9.1->-r ./requirements.txt (line 1)) (3.9.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/lazuli/.local/lib/python3.10/site-packages (from tensorflow==2.9.1->-r ./requirements.txt (line 1)) (0.4.0)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from tensorflow==2.9.1->-r ./requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/lazuli/.local/lib/python3.10/site-packages (from tensorflow==2.9.1->-r ./requirements.txt (line 1)) (1.1.2)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/lib/python3/dist-packages (from tensorflow==2.9.1->-r ./requirements.txt (line 1)) (3.12.4)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/lazuli/.local/lib/python3.10/site-packages (from tensorflow==2.9.1->-r ./requirements.txt (line 1)) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow==2.9.1->-r ./requirements.txt (line 1)) (59.6.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /home/lazuli/.local/lib/python3.10/site-packages (from tensorflow==2.9.1->-r ./requirements.txt (line 1)) (2.9.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/lazuli/.local/lib/python3.10/site-packages (from scikit-image==0.20.0->-r ./requirements.txt (line 2)) (2023.8.12)\n",
      "Requirement already satisfied: networkx>=2.8 in /home/lazuli/.local/lib/python3.10/site-packages (from scikit-image==0.20.0->-r ./requirements.txt (line 2)) (3.1)\n",
      "Requirement already satisfied: scipy>=1.8 in /home/lazuli/.local/lib/python3.10/site-packages (from scikit-image==0.20.0->-r ./requirements.txt (line 2)) (1.11.1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /home/lazuli/.local/lib/python3.10/site-packages (from scikit-image==0.20.0->-r ./requirements.txt (line 2)) (0.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/lazuli/.local/lib/python3.10/site-packages (from scikit-image==0.20.0->-r ./requirements.txt (line 2)) (1.4.1)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /home/lazuli/.local/lib/python3.10/site-packages (from scikit-image==0.20.0->-r ./requirements.txt (line 2)) (2.31.1)\n",
      "Requirement already satisfied: Shapely in /home/lazuli/.local/lib/python3.10/site-packages (from imgaug->-r ./requirements.txt (line 5)) (2.0.1)\n",
      "Requirement already satisfied: matplotlib in /home/lazuli/.local/lib/python3.10/site-packages (from imgaug->-r ./requirements.txt (line 5)) (3.7.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.1->-r ./requirements.txt (line 1)) (0.37.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/lazuli/.local/lib/python3.10/site-packages (from matplotlib->imgaug->-r ./requirements.txt (line 5)) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/lazuli/.local/lib/python3.10/site-packages (from matplotlib->imgaug->-r ./requirements.txt (line 5)) (4.38.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/lazuli/.local/lib/python3.10/site-packages (from matplotlib->imgaug->-r ./requirements.txt (line 5)) (0.10.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->imgaug->-r ./requirements.txt (line 5)) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/lazuli/.local/lib/python3.10/site-packages (from matplotlib->imgaug->-r ./requirements.txt (line 5)) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/lazuli/.local/lib/python3.10/site-packages (from matplotlib->imgaug->-r ./requirements.txt (line 5)) (1.0.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/lazuli/.local/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1->-r ./requirements.txt (line 1)) (2.28.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/lazuli/.local/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1->-r ./requirements.txt (line 1)) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/lazuli/.local/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1->-r ./requirements.txt (line 1)) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/lazuli/.local/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1->-r ./requirements.txt (line 1)) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1->-r ./requirements.txt (line 1)) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/lazuli/.local/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1->-r ./requirements.txt (line 1)) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/lazuli/.local/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1->-r ./requirements.txt (line 1)) (2.3.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/lazuli/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1->-r ./requirements.txt (line 1)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/lazuli/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1->-r ./requirements.txt (line 1)) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/lazuli/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1->-r ./requirements.txt (line 1)) (1.26.16)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/lazuli/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1->-r ./requirements.txt (line 1)) (5.3.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/lazuli/.local/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1->-r ./requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lazuli/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1->-r ./requirements.txt (line 1)) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/lazuli/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1->-r ./requirements.txt (line 1)) (2.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/lazuli/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1->-r ./requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/lazuli/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1->-r ./requirements.txt (line 1)) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/lazuli/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1->-r ./requirements.txt (line 1)) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1->-r ./requirements.txt (line 1)) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7-gGVFpTN5l"
   },
   "source": [
    "Import MaskRCNN libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "3iH6FrM-TOJ8"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Dataset' from 'mrcnn.Dataset' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmrcnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmrcnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MaskRCNN\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmrcnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mUtils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m visualize\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Dataset' from 'mrcnn.Dataset' (unknown location)"
     ]
    }
   ],
   "source": [
    "from mrcnn.Dataset import \n",
    "from mrcnn.model import MaskRCNN\n",
    "from mrcnn.Utils import visualize\n",
    "from mrcnn.Configs import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvMmP3dIRw4U"
   },
   "source": [
    "Common libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install roboflow\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgMNM03rMofc"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6pfim_EMpKQ"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMb_08d7PMRE"
   },
   "source": [
    "Download from Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rZfpHUEgPRjN",
    "outputId": "ba7f6141-4285-4e66-f42f-0f4d78fd15f5"
   },
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"sDHrdDWd5USqSBR0E7on\")\n",
    "project = rf.workspace(\"medusas\").project(\"alitas-tcc_h\")\n",
    "dataset = project.version(11).download(\"coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0UFconoPnqd"
   },
   "outputs": [],
   "source": [
    "DATA_SET_NAME = dataset.name.replace(\" \", \"-\")\n",
    "DATA_SET_LOCATION = dataset.location\n",
    "ANNOTATIONS_FILE_NAME = \"_annotations.coco.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dgRpgPiPea2"
   },
   "source": [
    "Dataset preprocessing (apply bilateral filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wQ_zMffxUjwg",
    "outputId": "1a26d242-0177-443b-b1a2-1e926c9b63ee"
   },
   "outputs": [],
   "source": [
    "def preprocess_coco_dataset(dataset_path, quiet=False):\n",
    "    BILATERAL_PARAMETERS = (2, 52, 84)\n",
    "\n",
    "    if not quiet:\n",
    "        print(f\"Preprocessing data on: {dataset_path}\")\n",
    "\n",
    "    img_filenames = os.listdir(dataset_path)\n",
    "    img_filenames.remove(\"_annotations.coco.json\")\n",
    "\n",
    "    for filename in img_filenames:\n",
    "        if not quiet:\n",
    "            print(f\"Applying Bilateral Filter on {filename}\")\n",
    "        try:\n",
    "            img = cv.imread(os.path.join(dataset_path, filename))\n",
    "            img = cv.bilateralFilter(img, *BILATERAL_PARAMETERS)\n",
    "            cv.imwrite(os.path.join(dataset_path, filename))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if not quiet:\n",
    "        print(f\"Data preprocessed successfully on {dataset_path}\")\n",
    "\n",
    "\n",
    "datasets_paths = [os.path.join(DATA_SET_LOCATION, 'train'),\n",
    "                 os.path.join(DATA_SET_LOCATION, 'test'),\n",
    "                 os.path.join(DATA_SET_LOCATION, 'valid'),]\n",
    "\n",
    "for dataset in datasets_paths:\n",
    "    preprocess_coco_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISZdVhfyVGf3"
   },
   "source": [
    "Prepare dataset for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-OU0i2cDlwt",
    "outputId": "aa71bc8d-64dc-477a-f538-386d49c8a029"
   },
   "outputs": [],
   "source": [
    "TEST_SET_PATH = os.path.join(DATA_SET_LOCATION, \"test\")\n",
    "\n",
    "dataset_test = load_images_dataset(os.path.join(TEST_SET_PATH, ANNOTATIONS_FILE_NAME), TEST_SET_PATH, \"test\")\n",
    "\n",
    "number_of_classes = dataset_test.count_classes()\n",
    "\n",
    "print('Test: ', len(dataset_test.image_ids))\n",
    "print('Class names: ', dataset_test.class_names)\n",
    "print(\"Classes: \", number_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9ghzXeXbn1u"
   },
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNKnZVQR1Ovq"
   },
   "source": [
    "### Configure network for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Die5FOLubgDA"
   },
   "source": [
    "Set output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jfs0l0owQSV1"
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR_PATH = os.path.join(\n",
    "    \"output\",\n",
    "    datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    ")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb37sKGrZc_0"
   },
   "source": [
    "Configure model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kSMeyzW1MRop",
    "outputId": "454a702f-0c37-4bd0-d6d1-877086c6781d"
   },
   "outputs": [],
   "source": [
    "cfg_kwargs = {\n",
    "    \"name\": \"alitas_tcc_h\",\n",
    "    \"num_classes\": number_of_classes,\n",
    "    \"interpolation_method\": \"bilinear\",\n",
    "    \"images_per_gpu\": 1\n",
    "}\n",
    "\n",
    "config = Config(**cfg_kwargs)\n",
    "model = MaskRCNN(mode=\"inference\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTa-HPTCBnqk",
    "outputId": "e8fb33ab-6d47-4e75-931b-61ae82f45fb3"
   },
   "outputs": [],
   "source": [
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_t1OFW_8zRNi"
   },
   "source": [
    "Load pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "thOHCzMIyv68",
    "outputId": "9fd67db5-f720-404b-88b8-fd51b5a20488"
   },
   "outputs": [],
   "source": [
    "PRE_TRAINED_WEIGHTS_PATH = \"pretrained_weights/mask_rcnn_alitas_tcc_h_0003.h5\"\n",
    "\n",
    "model.load_weights(filepath=PRE_TRAINED_WEIGHTS_PATH, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eW0VX7JO1Dan"
   },
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yE1QPKsc2ZRE"
   },
   "source": [
    "perform inference on test dataset with created predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJQCC1syO7EY",
    "outputId": "7481a83b-f504-4c6c-d005-c8d9f1ff7f88"
   },
   "outputs": [],
   "source": [
    "def inference_on_dataset(dataset, quiet=False):\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    for img_id in dataset_test.image_ids:\n",
    "        img_path = dataset_test.image_info[img_id].get(\"path\")\n",
    "\n",
    "        img = cv.imread(img_path)\n",
    "        img = img[:,:,::-1]\n",
    "        \n",
    "        if not quiet:\n",
    "            print(f\"running inference on: {img_path}\")\n",
    "        \n",
    "        detection = model.detect([img], verbose=0)[0]\n",
    "        predictions[img_path] = detection\n",
    "    \n",
    "    return predictions \n",
    "    \n",
    "predictions = inference_on_dataset(dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRVenpunRcv3"
   },
   "source": [
    "## Measuring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jelq36L75Zrn"
   },
   "source": [
    "Define measuring function to calculate biggest diagonal using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKBp0JWXQYCk"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def calc_longest_diagonal_pca(contour):\n",
    "\n",
    "    contour = np.squeeze(contour)\n",
    "\n",
    "    # handle single-point contours\n",
    "    if (len(contour.shape) == 1):\n",
    "        return tuple(contour), tuple(contour), 0\n",
    "\n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(contour)\n",
    "\n",
    "    principal_component = pca.components_[0]\n",
    "    contour_pca = np.dot(contour, principal_component)\n",
    "\n",
    "    start_index = np.argmin(contour_pca)\n",
    "    end_index = np.argmax(contour_pca)\n",
    "\n",
    "    start, end = contour[start_index], contour[end_index]\n",
    "    start, end = tuple(start), tuple(end)\n",
    "    length = math.dist(start, end)\n",
    "\n",
    "    return start, end, length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pE3rjfvR5qWj"
   },
   "source": [
    "Calculate longest diagonal with defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M65OG0085YXP"
   },
   "outputs": [],
   "source": [
    "def get_all_diagonals(predictions):\n",
    "    diagonals = {}\n",
    "\n",
    "    for (img_file_path, prediction) in predictions.items():\n",
    "\n",
    "        masks = prediction[\"masks\"]\n",
    "\n",
    "        debug_weird_imgs = [] # debug\n",
    "        diagonals_of_img_masks = []\n",
    "\n",
    "        masks = np.where(masks == 1, 255, 0)\n",
    "        masks = masks.astype(np.uint8)\n",
    "\n",
    "        for mask in np.moveaxis(masks, -1, 0):\n",
    "            contours,_ = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_NONE)\n",
    "            \n",
    "            # get the contour with the most points\n",
    "            contour = max(contours, key=lambda x: x.shape[0])\n",
    "            diagonal = calc_longest_diagonal_pca(contour)\n",
    "            diagonals_of_img_masks.append(diagonal)\n",
    "\n",
    "        diagonals[img_file_path] = diagonals_of_img_masks\n",
    "        \n",
    "    return diagonals\n",
    "\n",
    "diagonals = get_all_diagonals(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4jN4rHcDIZo"
   },
   "source": [
    "Display measured diagonals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WMs5OzYIrNfN",
    "outputId": "bc7e8dc5-4122-42d2-ffb6-c8a63b2e7f78"
   },
   "outputs": [],
   "source": [
    "IMAGES_SCALE_PX2NM = 0.8315\n",
    "\n",
    "def draw_diagonal_on_img(img, diag_start, diag_end, color=(200,200,200), thickness=2):\n",
    "    cv.line(img, diag_start, diag_end, color, thickness)\n",
    "\n",
    "for (img_file_path, prediction) in predictions.items():\n",
    "\n",
    "    img = cv.imread(img_file_path)\n",
    "    masks = prediction[\"masks\"]\n",
    "    diagonals_img = diagonals[img_file_path]\n",
    "\n",
    "    for i in range(masks.shape[-1]):\n",
    "        start, end, length_pixels = diagonals_img[i]\n",
    "\n",
    "        img_copy = np.array(img)\n",
    "        draw_diagonal_on_img(img_copy, start, end)\n",
    "        print(\"diagonal length (px): \", length_pixels)\n",
    "        length_in_nanometers = length_pixels * IMAGES_SCALE_PX2NM\n",
    "        print(\"diagonal length (nm): \", length_in_nanometers)\n",
    "\n",
    "        display_kwargs = {\n",
    "            \"image\": img_copy,\n",
    "            \"boxes\": prediction[\"rois\"],\n",
    "            \"masks\": prediction[\"masks\"],\n",
    "            \"class_ids\": prediction[\"class_ids\"],\n",
    "            \"class_names\": [\"\" for _ in range(len(prediction[\"class_ids\"]))],\n",
    "            # \"scores\": prediction[\"scores\"],\n",
    "        }\n",
    "\n",
    "        visualize.display_instances(**display_kwargs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
